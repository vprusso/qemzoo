{
  "description": [
    "Learning-Based Methods encompass a broad family of quantum error mitigation approaches that use classical machine learning to learn the effect of noise on quantum circuits and correct for it. These methods are data-driven: they train on circuits with known outputs to build a correction model, then apply it to target circuits.",
    "Approaches range from simple linear regression (as in CDR) to neural networks and kernel methods. The training data typically comes from circuits that are classically simulable (e.g., Clifford circuits, small circuits, or circuits with known symmetries).",
    "The appeal of learning-based methods is their flexibility: they can implicitly capture complex noise correlations without requiring an explicit noise model. The challenge is ensuring that the training distribution is representative of the target circuit's noise behavior."
  ],
  "how_it_works": [
    "Generate a set of training circuits with known ideal expectation values. Common sources: Clifford circuits (classically simulable), circuits with known symmetries, or small circuits that can be exactly simulated.",
    "Run each training circuit on the noisy quantum device and record the noisy expectation value.",
    "Train a classical model \\(f\\) on pairs \\((\\langle O \\rangle_{\\text{noisy}}, \\langle O \\rangle_{\\text{ideal}})\\). The model can be linear regression, polynomial regression, neural network, or any supervised learning method.",
    "Apply \\(f\\) to the noisy expectation value of the target circuit: \\(\\langle O \\rangle_{\\text{mitigated}} = f(\\langle O \\rangle_{\\text{noisy}})\\)."
  ],
  "key_equations": [
    {
      "label": "General learning model",
      "latex": "\\langle O \\rangle_{\\text{mitigated}} = f_\\theta(\\langle O \\rangle_{\\text{noisy}})"
    },
    {
      "label": "Training loss",
      "latex": "\\mathcal{L}(\\theta) = \\sum_{i=1}^{N_{\\text{train}}} \\left( f_\\theta(\\langle O \\rangle_{\\text{noisy}}^{(i)}) - \\langle O \\rangle_{\\text{ideal}}^{(i)} \\right)^2"
    }
  ],
  "bias_variance": {
    "bias": 0.45,
    "variance": 0.3,
    "overhead": 0.3,
    "notes": "Bias is dominated by model expressivity and distributional shift between training and target circuits. Variance is typically low at inference. Overhead comes from training circuit executions."
  },
  "advantages": [
    "No explicit noise model needed â€” the noise effect is learned from data",
    "Flexible: can capture complex noise correlations",
    "Low overhead at inference time",
    "Can leverage advances in classical ML (better models, more data)"
  ],
  "disadvantages": [
    "Bias from distributional shift: training circuits may not capture target circuit noise",
    "Requires quantum device time for generating training data",
    "Model selection and hyperparameter tuning add complexity",
    "Theoretical guarantees are generally weaker than for PEC or ZNE"
  ],
  "use_cases": [
    "Scenarios where explicit noise characterization is impractical",
    "Variational algorithms with structured ansatze",
    "Combining with noise-scaled data (vnCDR approach)",
    "Large-scale circuits where per-gate noise models are unavailable"
  ]
}
